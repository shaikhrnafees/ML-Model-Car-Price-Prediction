# -*- coding: utf-8 -*-
"""Car_predict.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tLKV_OV0gXwh72f5fZ2hq0YN6FzWrat5

# **STEP 1: Problem Statement**
The task involves predicting car selling prices using the "cardekho" dataset, which includes various features such as name, year, km_driven, fuel, seller_type, transmission, owner, mileage, engine, max_power, and seats. The problem statement is to build a machine learning model that accurately predicts the selling prices based on these car features and histories.

The initial steps include cleaning the dataset by handling missing values, correcting data types, and addressing outliers to ensure data quality. Exploratory Data Analysis (EDA) will be conducted to understand the relationships between the features and the target variable, selling_price, using visualizations to uncover patterns and correlations.

Data preprocessing will involve encoding categorical variables (e.g., fuel, seller_type, transmission, owner) and scaling numerical features (e.g., km_driven, mileage, engine, max_power). After preprocessing, a machine learning model, such as RandomForestRegressor, will be created to predict selling prices. Hyperparameter tuning will be performed to optimize the model’s performance, ensuring accurate and reliable predictions based on the given car features and histories.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns  #Nafees

"""# **Step 2: Data Collection**


"""

df=pd.read_csv("/content/cardekho.csv")

df.head()

# Extract the first part from the 'name' column because we need only the brand name
df['name'] = df['name'].apply(lambda x: x.split()[0])

df.head(5)

df.info()  # as you can see there are object columns according to dataframe

df.shape # rows,columns

df.describe() # some important information(mean,std, count , outliers view)

df.isna().sum() # null values lets check the weightage is higher 50 % or not

(df.isna().sum()/len(df))*100  # less than 50% so we can not drop

# Heatmap for showing missing values in data
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

"""# **Step 3: Data Preprocessing - Part 1**"""

from sklearn.impute import SimpleImputer

# filling the missing values with mode because i cant take other method to justify this most frequent is suitable
s=SimpleImputer(strategy='most_frequent')
df[['mileage(km/ltr/kg)', 'engine', 'max_power', 'seats']] = s.fit_transform(df[['mileage(km/ltr/kg)', 'engine', 'max_power', 'seats']])

df.isna().sum() # now no missing values

df.duplicated().sum()

df.drop_duplicates(keep='first',inplace=True)

df.duplicated().sum() # no more duplicates

df.shape # rows,columns

# Now checking how much the data we cleaned
data_cleaned=round(((8128-6907)/8128)*100,1)
print(f"Cleaned Data = {data_cleaned} %")

df.head()

"""# **Step 4: Exploratory Data Analysis (EDA)**"""

# Distributor counts in car variants

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='fuel')
plt.title('Distribution of Fuel Types')
plt.xlabel('Fuel Type')
plt.ylabel('Count')
plt.show()

# Distribution of Seller Types

plt.figure(figsize=(8, 8))
df['seller_type'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90)
plt.title('Distribution of Seller Types')
plt.ylabel('')
plt.show()

# Transmission Types by Fuel

plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='fuel', hue='transmission')
plt.title('Transmission Types by Fuel')
plt.xlabel('Fuel Type')
plt.ylabel('Count')
plt.legend(title='Transmission')
plt.show()

# Selling Price vs. Mileage

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='mileage(km/ltr/kg)', y='selling_price')
plt.title('Selling Price vs. Mileage')
plt.xlabel('Mileage (km/ltr/kg)')
plt.ylabel('Selling Price')
plt.show()

# Selling Price by Transmission Type
# There are outliers here but according to data we con not remove them because some car might be high price some care has low price.
# Example = BMW Car >>> Maruti Cars
plt.figure(figsize=(10, 6))
sns.boxplot(data=df, x='transmission', y='selling_price')
plt.title('Selling Price by Transmission Type')
plt.xlabel('Transmission Type')
plt.ylabel('Selling Price')
plt.show()

"""# **Step 3: Data Preprocessing- Part 2**"""

df.nunique() # unique values in each columns

df["owner"].unique()

# Map Encoding to justify the value

custom_encoding = {
    'First Owner': 1,
    'Second Owner': 2,
    'Third Owner': 3,
    'Fourth & Above Owner': 4,
    'Test Drive Car': 5
}

# Replace values in 'owner' column using custom encoding
df['owner'] = df['owner'].map(custom_encoding)

df["fuel"].unique()

custom_encoding_fuel = {
    'Diesel': 1,
    'Petrol': 2,
    'LPG': 3,
    'CNG' : 4,
}

df['fuel'] = df['fuel'].map(custom_encoding_fuel)

df["seller_type"].unique()

custom_encoding_seller = {
    'Individual': 1,
    'Dealer': 2,
    'Trustmark Dealer': 3
}
df['seller_type'] = df['seller_type'].map(custom_encoding_seller)

df['transmission'].unique()

custom_encoding_transmission = {
    'Manual': 2,
    'Automatic': 1,
}
df['transmission'] = df['transmission'].map(custom_encoding_transmission)

df['name'].unique()

from sklearn.preprocessing import LabelEncoder # Now doing the lable coding for higher unique values for this Lable Encoder is perfect fit.

label_encoder = LabelEncoder()

df['name'] = label_encoder.fit_transform(df['name'])

label_encoder = LabelEncoder()

df['mileage(km/ltr/kg)'] = label_encoder.fit_transform(df['mileage(km/ltr/kg)'])

label_encoder = LabelEncoder()

df['engine'] = label_encoder.fit_transform(df['engine'])

label_encoder = LabelEncoder()

df['max_power'] = label_encoder.fit_transform(df['max_power'])

label_encoder = LabelEncoder()

df['seats'] = label_encoder.fit_transform(df['seats'])

df.head(2) # Labeling Completed

df.info() # before and after info now we have all int datatype

# Checking relations

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f', annot_kws={'size': 10})
plt.title('Correlation Heatmap')
plt.show()

"""# **Step 5: Model Selection, Training & Evaluation**"""

# Deviding data to dependent and independent variable

input_data=df.drop('selling_price',axis=1)
output_data=df['selling_price']

from sklearn.preprocessing import StandardScaler

# Scaling the data to handle imbalance data.

ss=StandardScaler()
input_data=ss.fit_transform(input_data)

from sklearn.model_selection import train_test_split

# diving data inton train and test both depnedent and independent with ration 80-20

x_train,x_test,y_train,y_test=train_test_split(input_data,output_data,test_size=0.2,random_state=42)

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score

rf = RandomForestRegressor()

# Define the hyperparameter grid to search only for n_estimators
param_grid = {
    'n_estimators': [50, 100, 150, 200]  # Hyperparameter tuning only for n_estimators
}

# Initialize GridSearchCV with 3-fold cross-validation for faster runtime
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

# Fit the grid search to the data
grid_search.fit(x_train, y_train)

# Print the best parameters found by GridSearchCV
print("Best parameters found: ", grid_search.best_params_)

# Use the best estimator to make predictions
best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(x_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")

# Create a DataFrame for actual vs predicted values
results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})

"""**R² Score:** An R² score of approximately 0.918 suggests that about 91.8% of the variance in selling prices is explained by the features included in the model. This is generally considered a good fit, indicating that the model captures a significant amount of the variability in selling prices.

**MSE: **The MSE value of approximately 18 billion indicates that, on average, the model’s predictions are off by around 4.27 million in terms of selling price. This could be considered acceptable depending on the price range and the business context.

# **Actucal vs Predicted Values Chart**
"""

plt.figure(figsize=(10, 6))
plt.scatter(results_df.index, results_df['Actual'], color='blue', label='Actual Selling Price')
plt.scatter(results_df.index, results_df['Predicted'], color='red', label='Predicted Selling Price')
plt.plot(results_df.index, results_df['Actual'], color='blue', alpha=0.5)
plt.plot(results_df.index, results_df['Predicted'], color='red', alpha=0.5)
plt.xlabel('Index')
plt.ylabel('Selling Price')
plt.title('Actual vs Predicted Selling Price')
plt.legend()
plt.show()

plt.figure(figsize=(12, 6))

# Actual Selling Price
plt.subplot(1, 2, 1)
plt.plot(results_df.index, results_df['Actual'], color='blue', label='Actual Selling Price')
plt.xlabel('Index')
plt.ylabel('Selling Price')
plt.title('Actual Selling Price')
plt.legend()

# Predicted Selling Price
plt.subplot(1, 2, 2)
plt.plot(results_df.index, results_df['Predicted'], color='red', label='Predicted Selling Price')
plt.xlabel('Index')
plt.ylabel('Selling Price')
plt.title('Predicted Selling Price')
plt.legend()

plt.tight_layout()
plt.show()

x=df.drop('selling_price',axis=1)
y=df['selling_price']
sample_index = 0

# Fit and transform data with StandardScaler
scaler = StandardScaler()
scaler.fit(x)
sample_data_scaled = scaler.transform(x.iloc[sample_index:sample_index+1])

# Trained RandomForestRegressor model
rf = RandomForestRegressor()
rf.fit(x, y)

# Predict using the trained model on scaled sample data
predicted_price = rf.predict(sample_data_scaled)

# AS we can see here the price diffrence in index 1 for the accuracy
print("Original Sample Data:")
print(x.iloc[sample_index])
print("\nPredicted Price for sample data at index", sample_index, ":", predicted_price)

df.head(1)  # check selling price now

"""# **Thank you for your time. I look forward to your response.**

**Nafees Shaikh**
"""